
<!-- Publications -->

<button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-publications" id="publications"><heading>Publications</heading></button>
<div id="content-publications" class="collapse in">

<table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src='images/hyway.jpg' width="190">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <papertitle><big>HyWay: Enabling Mingling in the Hybrid World</papertitle></big>
            <br>
            <p>
                Harsh Vijay, Saumay Pushp, <strong>Amish Mittal</strong>, Praveen, ... , Ajay Manchepalli, Venkat Padmanabhan
                <br>
                <strong>UbiComp</strong>/IMWUT 2023
                <br>
                <em><a href="https://dl.acm.org/doi/abs/10.1145/3596235" target="_blank">[paper]</a></em>
            </p>
            <p>
                We present HyWay, short for "Hybrid Hallway", to enable mingling and informal 
                interactions among physical and virtual users, in casual spaces and settings, 
                such as office water cooler areas, conference hallways, trade show floors, and more.
                Key to the design of HyWay is bridging the awareness gap between physical and virtual 
                users, and providing the virtual users the same agency as physical users.
                <!-- Results from our deployments show that HyWay enables effective 
                mingling. -->
            </p>
            <p><small>*Author order reverse alphabetical
            </small></p>
        </td>
    </tr>
    
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src='images/ScienceQA.png' width="150">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <papertitle><big>ScienceQA: A Novel Benchmark Resource for Question Answering on Scholarly Articles</papertitle></big>
            <br>
            <p>
                Tanik Saikh, Tirthankar Ghosal, <strong>Amish Mittal</strong>, Asif Ekbal, Pushpak Bhattacharyya
                <br>
                International Journal of Digital Libraries (<strong>IJDL</strong>), 2022
                <br>
                <em><a href="https://link.springer.com/article/10.1007/s00799-022-00329-y" target="_blank">[paper]</a></em>
            </p>
            <p>
                We introduce a semi-automated dataset having more than 100k 
                human-annotated context-question-answer triplets to facilitate question answering 
                (QA) on scientific articles. Secondly, we implement 
                QA models based on BERT, SciBERT and a combination of
                SciBERT and BiDAF and evaluate the results on our dataset. The
                best model obtains an F1 score of 75.46%.
            </p>
        </td>
    </tr>

    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src='images/ad_text_model.png' width="150">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <papertitle><big>Multi-Modal Detection of Alzheimer's Disease from Speech and Text</papertitle></big>
            <br>
            <p>
                <strong>Amish Mittal*</strong>, Sourav Sahoo*, Arnhav Datar*, Juned Kadiwala*, Hrithwik Shalu, Jimson Mathew
                <br>
            20th International Workshop on Data Mining in Bioinformatics (<strong>BIOKDD</strong>) with SIGKDD 2021
            <br>
            <em><a href="https://arxiv.org/abs/2012.00096" target="_blank">[paper]</a></em></p>
            <p>
                Reliable detection of the prodromal stages of Alzheimer's disease (AD) remains difficult even today because there is no definitive diagnosis of AD in vivo.
                We propose a multimodal deep learning method that utilizes speech and the corresponding transcript simultaneously to detect AD.
                <!-- We also perform experiments to analyze the model performance when ASR system generated transcripts are used and further perform an essential study of age and gender bias of our model.  -->
                The proposed method achieves 85.3% 10-fold cross-validation accuracy on the Dementiabank Pitt corpus.
            </p>
            <p><small>*Authors contributed equally
                <br>
                In collaboration with <b> JCBC, University of Cambridge, UK</b>
            </small></p>
        </td>
    </tr>

</tbody></table>
</div>
<hr class="soft">